{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faea2791",
   "metadata": {},
   "source": [
    "# Python Course - Tutorial 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb82258",
   "metadata": {},
   "source": [
    "### Exercise 1 (Sharpe Ratio Comparison)\n",
    "In the field of finance, the **Sharpe Ratio** is a common metric used to understand how well an investment performs relative to its risk. Formally, the Sharpe Ratio is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Sharpe Ratio} = \\frac{R_p - R_f}{\\sigma_p}\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $R_p$ is the expected return (often taken as the mean return of the asset over a given period).  \n",
    "- $R_f$ is the risk-free rate (a theoretical return of an investment with zero risk, often approximated by government bonds).  \n",
    "- $\\sigma_p$ is the standard deviation (volatility) of the asset’s returns.\n",
    "\n",
    "A higher Sharpe Ratio indicates that the investment is providing a higher return per unit of risk.\n",
    "\n",
    "---\n",
    "\n",
    "**The datasets required for this exercise are provided in the `data/` directory:**\n",
    "\n",
    "- `data/SP500_Total_Return.csv`: Historical data for the S&P 500 Total Return Index  \n",
    "- `data/DAX.csv`: Historical data for the DAX Index  \n",
    "\n",
    "**Your tasks are to:**  \n",
    "\n",
    "(i) Load both datasets from the CSV files into Pandas DataFrames.  \n",
    "\n",
    "(ii) Compute the daily returns for each index (use e.g., Pandas `pct_change()` method).  \n",
    "\n",
    "(iii) Assume a **constant annualized risk-free rate of 2%** for this analysis. (In practice, you might load this from a dataset or a more appropriate proxy.)  \n",
    "\n",
    "(iv) Compute the annualized Sharpe Ratio for each index using:\n",
    "\n",
    "$$\n",
    "\\text{Sharpe Ratio (annualized)} = \\frac{(\\text{mean daily return} - \\text{daily risk-free rate}) \\times 252}{\\text{daily return volatility} \\times \\sqrt{252}}\n",
    "$$\n",
    "\n",
    "Here, $252$ represents the approximate number of trading days in a year.\n",
    "\n",
    "(v) Print out the Sharpe Ratio for both the S&P 500 Total Return and the DAX, and compare which one has a higher risk-adjusted return.\n",
    "\n",
    "**Note:** If a particular index has no data or returns are NaN, handle it gracefully and report that the Sharpe Ratio cannot be computed."
   ]
  },
  {
   "cell_type": "code",
   "id": "9a6c03e1",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# (i) Load the CSV files into DataFrames\n",
    "sp500_df = pd.read_csv(\"data/SP500_Total_Return.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "dax_df = pd.read_csv(\"data/DAX.csv\", parse_dates=[\"Date\"], index_col=\"Date\")\n",
    "\n",
    "# (ii) Compute daily returns\n",
    "sp500_df[\"Daily_Return\"] = sp500_df[\"Adj Close\"].pct_change()\n",
    "dax_df[\"Daily_Return\"] = dax_df[\"Adj Close\"].pct_change()\n",
    "\n",
    "# (iii) Assume an annualized risk-free rate of 2%\n",
    "annual_rf = 0.02\n",
    "# Convert annual risk-free rate to daily\n",
    "daily_rf = (1 + annual_rf)**(1/252) - 1\n",
    "\n",
    "# (iv) Compute Sharpe Ratios\n",
    "def compute_annualized_sharpe_ratio(returns, daily_risk_free):\n",
    "    # Drop NaN values\n",
    "    returns = returns.dropna()\n",
    "    if len(returns) == 0:\n",
    "        return np.nan\n",
    "    mean_daily = returns.mean()\n",
    "    daily_vol = returns.std()\n",
    "    # Annualized Sharpe Ratio\n",
    "    sharpe_ratio = ((mean_daily - daily_risk_free) * 252) / (daily_vol * np.sqrt(252))\n",
    "    return sharpe_ratio\n",
    "\n",
    "sp500_sharpe = compute_annualized_sharpe_ratio(sp500_df[\"Daily_Return\"], daily_rf)\n",
    "dax_sharpe = compute_annualized_sharpe_ratio(dax_df[\"Daily_Return\"], daily_rf)\n",
    "\n",
    "# (v) Print out and compare\n",
    "print(\"S&P 500 Total Return Sharpe Ratio:\", round(sp500_sharpe, 4))\n",
    "print(\"DAX Sharpe Ratio:\", round(dax_sharpe, 4))\n",
    "if pd.isna(sp500_sharpe):\n",
    "    print(\"Cannot compute Sharpe Ratio for S&P 500 Total Return\")\n",
    "if pd.isna(dax_sharpe):\n",
    "    print(\"Cannot compute Sharpe Ratio for DAX\")\n",
    "if not pd.isna(sp500_sharpe) and not pd.isna(dax_sharpe):\n",
    "    if sp500_sharpe > dax_sharpe:\n",
    "        print(\"S&P 500 Total Return has a higher risk-adjusted return.\")\n",
    "    else:\n",
    "        print(\"DAX has a higher risk-adjusted return.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "792a7cec",
   "metadata": {},
   "source": [
    "### Exercise 2: Tidy Data\n",
    "\n",
    "#### Overview: Wickham's Tidy Data Framework\n",
    "Hadley Wickham's seminal paper on [Tidy Data](https://www.jstatsoft.org/article/view/v059i10) focuses on structuring datasets to facilitate analysis. The principles are:\n",
    "1. **Each variable forms a column.**\n",
    "2. **Each observation forms a row.**\n",
    "3. **Each type of observational unit forms a table.**\n",
    "\n",
    "Tidy datasets ensure a consistent structure, simplifying manipulation, visualization, and modeling. However, real-world datasets often deviate from this structure, requiring transformation.\n",
    "\n",
    "#### Common Problems in Untidy Data\n",
    "Wickham identifies five types of untidy data:\n",
    "1. **Column headers are values, not variable names.**\n",
    "2. **Multiple variables are stored in one column.**\n",
    "3. **Variables are stored in both rows and columns.**\n",
    "4. **Multiple types of observational units are stored in the same table.**\n",
    "5. **One type of observational unit is spread out over multiple tables or files**"
   ]
  },
  {
   "cell_type": "code",
   "id": "00ef5773",
   "metadata": {},
   "source": [
    "# Importing pandas\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe3a9659",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Column Headers Are Values, Not Variable Names - Part 1\n",
    "\n",
    "In the Pew dataset column headers represent income brackets rather than variable names. Our goal is to restructure the data into a tidy format, with columns for `religion`, `income`, and `frequency`.\n",
    "\n",
    "#### Tasks:\n",
    "1. Load the Pew dataset from a CSV file.\n",
    "2. Inspect the data to identify untidy elements.\n",
    "3. Use Pandas' `melt` function to:\n",
    "   - Transform the income brackets (column headers) into a new column named `income`.\n",
    "   - Ensure the original `religion` column remains intact.\n",
    "   - Create a new column named `frequency` to hold the corresponding values.\n",
    "4. Sort the resulting tidy dataset by `religion` (alphabetically) and `frequency` (numerically).\n",
    "\n",
    "\n",
    "#### Dataset Preview:\n",
    "\n",
    "The untidy data might look like this:\n",
    "\n",
    "| religion           | \\<$10k | \\$10–20k | \\$20–30k | \\$30–40k | \\$40–50k | \\$50–75k |\n",
    "|--------------------|--------|---------|---------|---------|---------|---------|\n",
    "| Agnostic           | 27     | 34      | 60      | 81      | 76      | 137     |\n",
    "| Atheist            | 12     | 27      | 37      | 52      | 35      | 70      |\n",
    "| Buddhist           | 27     | 21      | 30      | 34      | 33      | 58      |\n",
    "| Catholic           | 418    | 617     | 732     | 670     | 638     | 1116    |\n",
    "| Don’t know/refused | 15     | 14      | 15      | 11      | 10      | 35      |\n",
    "\n",
    "Using `pd.melt`, we can achieve the following tidy dataset:\n",
    "\n",
    "| religion           | income   | frequency |\n",
    "|--------------------|----------|-----------|\n",
    "| Agnostic           | \\<\\$10k  | 27        |\n",
    "| Agnostic           | \\$10–20k | 34        |\n",
    "| Agnostic           | \\$20–30k | 60        |\n",
    "| Atheist            | \\<\\$10k  | 12        |\n",
    "| Atheist            | \\$10–20k | 27        |\n",
    "| Buddhist           | \\<\\$10k  | 27        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "06d75fae",
   "metadata": {},
   "source": [
    "# Import the Pew sample dataset\n",
    "pew_data = pd.read_csv(\"data/pew_sample.csv\")\n",
    "\n",
    "# Reshape the Pew dataset from wide format to long format\n",
    "pew_long_format = pd.melt(\n",
    "    frame=pew_data,\n",
    "    id_vars=[\"religion\"],  # Keep the \"religion\" column fixed\n",
    "    value_vars=pew_data.columns[1:],  # Use all other columns as value variables\n",
    "    var_name=\"income_bracket\",  \n",
    "    value_name=\"frequency\",  \n",
    ")\n",
    "\n",
    "# Sort the reshaped data by religion and income_bracket\n",
    "pew_long_sorted = pew_long_format.sort_values(by=[\"religion\", \"income_bracket\"])\n",
    "\n",
    "# Display the first rows of sorted data\n",
    "pew_long_sorted.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "66ddf41e",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Column Headers Are Values, Not Variable Names - Part 2\n",
    "\n",
    "The **Billboard dataset** is stored in a wide format. The data contains weekly rankings (`wk1`, `wk2`, ..., `wk75`) spread across columns. Your task is to transform it into a **long format** where each row corresponds to a specific song's ranking in a particular week.\n",
    "\n",
    "#### Tasks:\n",
    "1. **Inspect the Dataset**: Familiarize yourself with the structure of the Billboard dataset. Identify columns to retain (e.g., `year`, `artist`, `track`, `time`, `date.entered`) and those to melt (e.g., `wk1`, `wk2`, ..., `wk75`).\n",
    "2. **Use `pd.melt`**:\n",
    "   - Reshape the dataset so that all weekly ranking columns (`wk1`, `wk2`, ..., `wk75`) are combined into a single `week` column.\n",
    "   - Create a new column `rank` to hold the corresponding ranking values.\n",
    "3. **Clean Up the Week Column**:\n",
    "   - Use string methods to extract only the numeric part of the week values (e.g., `wk1` → `1`).\n",
    "   - Convert the `week` column into an integer type for easier manipulation.\n",
    "\n",
    "**Hint:** Apply the `.str.replace()` or `.str.extract()` methods to clean up the `week` column.\n",
    "\n",
    "#### Dataset Preview:\n",
    "**Original Untidy Data (Wide Format)**:\n",
    "\n",
    "| year | artist       | track                    | time | date.entered | wk1 | wk2 | wk3 | ... | wk75 |\n",
    "|------|--------------|--------------------------|------|--------------|-----|-----|-----|-----|------|\n",
    "| 2000 | 2 Pac        | Baby Don’t Cry           | 4:22 | 2000-02-26   | 87  | 82  | 72  | ... | NaN  |\n",
    "| 2000 | 2Ge+her      | The Hardest Part Of ...  | 3:15 | 2000-09-02   | 91  | 87  | 82  | ... | NaN  |\n",
    "| 2000 | 3 Doors Down | Kryptonite               | 3:53 | 2000-04-08   | 81  | 70  | 66  | ... | NaN  |\n",
    "\n",
    "\n",
    "**After Transformation (Long Format)**:\n",
    "\n",
    "| year | artist       | time | track                  | date       | week | rank |\n",
    "|------|--------------|------|------------------------|------------|------|------|\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-02-26 | 1    | 87   |\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-03-04 | 2    | 82   |\n",
    "| 2000 | 2 Pac        | 4:22 | Baby Don’t Cry         | 2000-03-11 | 3    | 72   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-02 | 1    | 91   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-09 | 2    | 87   |\n",
    "| 2000 | 2Ge+her      | 3:15 | The Hardest Part Of ...| 2000-09-02 | 3    | 92   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-08 | 1    | 81   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-15 | 2    | 70   |\n",
    "| 2000 | 3 Doors Down | 3:53 | Kryptonite             | 2000-04-22 | 3    | 66   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "615f65b7",
   "metadata": {},
   "source": [
    "# Import the Billboard dataset\n",
    "billboard_data = pd.read_csv(\"data/billboard.csv\")\n",
    "\n",
    "# Define columns to keep fixed during the transformation\n",
    "id_columns = [\n",
    "    \"year\",\n",
    "    \"artist.inverted\",\n",
    "    \"track\",\n",
    "    \"time\",\n",
    "    \"genre\",\n",
    "    \"date.entered\",\n",
    "    \"date.peaked\",\n",
    "]\n",
    "\n",
    "# Reshape the dataset from wide to long format\n",
    "# Columns not in id_columns will become the 'week' column in the resulting DataFrame\n",
    "billboard_long_format = pd.melt(\n",
    "    frame=billboard_data,\n",
    "    id_vars=id_columns,\n",
    "    var_name=\"week\",  # Name for the variable column (week number column)\n",
    "    value_name=\"rank\",  # Name for the value column (chart rank)\n",
    ")\n",
    "\n",
    "# Clean the 'week' column by removing unwanted text patterns and convert to integer\n",
    "billboard_long_format[\"week\"] = billboard_long_format[\"week\"].str.replace(\n",
    "    \"x|st.week|th.week|nd.week|rd.week\", \"\", regex=True\n",
    ").astype(int)\n",
    "\n",
    "# Calculate the actual chart date by adding the week offset to the 'date.entered'\n",
    "billboard_long_format[\"date\"] = pd.to_datetime(billboard_long_format[\"date.entered\"]) + pd.to_timedelta(\n",
    "    (billboard_long_format[\"week\"] - 1) * 7, \"d\"\n",
    ")\n",
    "\n",
    "# Sort the dataset by track and date for better readability\n",
    "billboard_long_sorted = billboard_long_format.sort_values(by=[\"track\", \"date\"])\n",
    "\n",
    "# Drop rows with missing values (e.g., ranks not recorded for certain weeks)\n",
    "billboard_cleaned = billboard_long_sorted.dropna()\n",
    "\n",
    "# Display the cleaned and processed DataFrame\n",
    "billboard_cleaned"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0065049",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Multiple Variables Are Stored in One Column\n",
    "\n",
    "In the tuberculosis (TB) dataset multiple variables (e.g., sex and age group) are combined into a single column. Your goal is to separate these into distinct columns, making the dataset tidy.\n",
    "\n",
    "\n",
    "#### Tasks:\n",
    "1. **Reshape the Dataset**: Use `pd.melt` to convert the wide format into a long format where the demographic variables (e.g., `m014`, `f1524`) are stored in a single column.\n",
    "2. **Split the Combined Column**: Extract the `sex` and `age` variables from the demographic column.\n",
    "3. **Clean the Age Values**: Use the `map` method with a dictionary to convert the age codes (e.g., `014`, `1524`) into human-readable ranges (e.g., `0–14`, `15–24`).\n",
    "\n",
    "\n",
    "#### Dataset Preview:\n",
    "**Original Untidy Data**:\n",
    "\n",
    "| country | year | m014 | m1524 | m2534 | m3544 | m4554 | m5564 | m65 | mu  | f014 | f1524 | f2534 |\n",
    "|---------|------|------|-------|-------|-------|-------|-------|-----|-----|------|-------|-------|\n",
    "| AD      | 2000 | 0    | 0     | 1     | 0     | 0     | 0     | 0   | --- | ---  | ---   | ---   |\n",
    "| AE      | 2000 | 2    | 4     | 4     | 6     | 5     | 12    | 10  | --- | 3    | 6     | 5     |\n",
    "| AF      | 2000 | 52   | 228   | 183   | 149   | 129   | 94    | 80  | --- | 93   | 142   | 128   |\n",
    "\n",
    "**After Transformation (Tidy Format)**:\n",
    "\n",
    "| country | year | sex | age  | cases |\n",
    "|---------|------|-----|------|-------|\n",
    "| AD      | 2000 | m   | 0–14 | 0     |\n",
    "| AE      | 2000 | m   | 0–14 | 2     |\n",
    "| AF      | 2000 | m   | 0–14 | 52    |\n",
    "| AD      | 2000 | f   | 15–24| 0     |\n",
    "| AE      | 2000 | f   | 15–24| 6     |\n",
    "| AF      | 2000 | f   | 15–24| 142   |\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b4a07ee",
   "metadata": {},
   "source": [
    "# Import the TB dataset\n",
    "tb_data = pd.read_csv(\"data/tb_sample.csv\")\n",
    "\n",
    "# Reshape the dataset from wide to long format\n",
    "tb_long_format = pd.melt(\n",
    "    tb_data,\n",
    "    id_vars=[\"country\", \"year\"],  # Keep \"country\" and \"year\" as fixed identifiers\n",
    "    value_vars=list(tb_data.columns)[2:],  # Columns to unpivot\n",
    "    var_name=\"column\",  \n",
    "    value_name=\"cases\",  \n",
    ")\n",
    "\n",
    "# Extract the 'sex' information from the 'column' field\n",
    "tb_long_format[\"sex\"] = tb_long_format[\"column\"].str[0]\n",
    "\n",
    "# Extract the 'age' information and map it to readable age groups\n",
    "tb_long_format[\"age\"] = tb_long_format[\"column\"].str[1:].map(\n",
    "    {\n",
    "        \"014\": \"0-14\",\n",
    "        \"1524\": \"15-24\",\n",
    "        \"2534\": \"25-34\",\n",
    "        \"3544\": \"35-44\",\n",
    "        \"4554\": \"45-54\",\n",
    "        \"5564\": \"55-64\",\n",
    "        \"65\": \"65+\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns after extracting useful information\n",
    "tb_cleaned = tb_long_format.drop(columns=[\"column\"])\n",
    "\n",
    "# Display the cleaned and formatted tb dataset\n",
    "tb_cleaned.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d4a0bd0",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Variables Are Stored in Both Rows and Columns (Bonus!)\n",
    "\n",
    "The weather dataset contains variables such as minimum and maximum temperature (`tmin`, `tmax`) that stored across both rows and columns. Your task is to tidy the dataset by ensuring each variable is represented in its own column.\n",
    "\n",
    "\n",
    "\n",
    "#### Tasks:\n",
    "1. **Reshape the Dataset**:\n",
    "   - Use `pd.melt` to gather all day columns (`d1`, `d2`, ..., `d31`) into a single column, converting the wide format into a long format.\n",
    "   - Create a new `date` column by combining `year`, `month`, and the day extracted from the melted column.\n",
    "\n",
    "2. **Separate Variables**:\n",
    "   - Pivot the data so that `tmin` and `tmax` are stored as separate columns, with each row representing a unique `date`.\n",
    "\n",
    "3. **Handle Missing Values**:\n",
    "   - Remove rows where the `value` is missing (`—`).\n",
    "\n",
    "**Hints:**\n",
    "- Combine `year`, `month`, and `day` into a single `date` column using `pd.to_datetime` or string formatting.\n",
    "- Use `pivot` or `unstack` to move the `element` values (`tmin`, `tmax`) into separate columns.\n",
    "\n",
    "#### Dataset Preview\n",
    "**Original Untidy Data**:\n",
    "\n",
    "| id      | year | month | element | d1   | d2   | d3   | ... | d8  |\n",
    "|---------|------|-------|---------|------|------|------|-----|------|\n",
    "| MX17004 | 2010 | 1     | tmax    | —    | —    | —    | ... | —    |\n",
    "| MX17004 | 2010 | 1     | tmin    | —    | —    | —    | ... | —    |\n",
    "| MX17004 | 2010 | 2     | tmax    | —    | 27.3 | 24.1 | ... | —    |\n",
    "| MX17004 | 2010 | 2     | tmin    | —    | 14.4 | 14.4 | ... | —    |\n",
    "\n",
    "**After Transformation (Tidy Format)**:\n",
    "\n",
    "| id      | date       | tmax  | tmin  |\n",
    "|---------|------------|-------|-------|\n",
    "| MX17004 | 2010-02-02 | 27.3  | 14.4  |\n",
    "| MX17004 | 2010-02-03 | 24.1  | 14.4  |\n",
    "| MX17004 | 2010-03-05 | 32.1  | 14.2  |"
   ]
  },
  {
   "cell_type": "code",
   "id": "1434c817",
   "metadata": {},
   "source": [
    "# Import the weather sample dataset\n",
    "weather_data = pd.read_csv(\"data/weather_sample.csv\")\n",
    "\n",
    "# Transform the data from wide format to long format\n",
    "weather_long = pd.melt(\n",
    "    frame=weather_data,\n",
    "    id_vars=[\"id\", \"year\", \"month\", \"element\"],\n",
    "    var_name=\"day\",\n",
    "    value_name=\"value\",\n",
    ")\n",
    "\n",
    "# Remove the leading \"d\" from the day column and convert to integer\n",
    "weather_long[\"day\"] = weather_long[\"day\"].str[1:].astype(\"int\")\n",
    "\n",
    "# Combine year, month, and day into a single date column in YYYY-MM-DD format\n",
    "weather_long[\"date\"] = weather_long[[\"year\", \"month\", \"day\"]].apply(\n",
    "    lambda row: \"{:4d}-{:02d}-{:02d}\".format(*row), axis=1\n",
    ")\n",
    "\n",
    "# Filter out rows with missing values and keep relevant columns\n",
    "weather_filtered = weather_long.loc[\n",
    "    ~weather_long[\"value\"].isna(), [\"id\", \"date\", \"element\", \"value\"]\n",
    "]\n",
    "\n",
    "# Set index to a multi-index of id, date, and element\n",
    "weather_indexed = weather_filtered.set_index([\"id\", \"date\", \"element\"])\n",
    "\n",
    "# Unstack the element level of the index to create separate columns for each element\n",
    "weather_wide = weather_indexed.unstack()\n",
    "\n",
    "# Flatten the multi-index column names\n",
    "weather_wide.columns = list(weather_wide.columns.get_level_values(\"element\"))\n",
    "\n",
    "# Reset the index to convert it back into a DataFrame\n",
    "weather_cleaned = weather_wide.reset_index()\n",
    "\n",
    "# Display the formatted dataframe\n",
    "weather_cleaned"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43b1e9ee",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Multiple Types of Observational Units Are Stored in the Same Table\n",
    "\n",
    "The Billboard dataset, contains information about tracks (e.g., artist, track title, and time) as well as their rankings over time. These represent two distinct types of observational units. Your goal is to split the dataset into two separate tables:\n",
    "1. One table for track information.\n",
    "2. Another table for weekly rankings.\n",
    "\n",
    "\n",
    "#### Steps to Complete:\n",
    "\n",
    "1. **Extract Track Information**: Identify the unique combinations of `artist`, `track`, and `time` and assign each unique combination a unique `id`.\n",
    "2. **Separate Weekly Rankings**: Create a new table for rankings, using the `id` from the track table to link the two datasets.\n",
    "3. **Join Tables When Needed**: Use the `merge` function to link track information with ranking data when required for analysis.\n",
    "\n",
    "**Hint:** Use `pd.drop_duplicates` to extract unique rows for the track table.\n",
    "\n",
    "\n",
    "#### Dataset Preview\n",
    "**Original Untidy Data** (Mixed Observational Units):\n",
    "\n",
    "| year | artist          | track              | time | date       | week | rank |\n",
    "|------|-----------------|--------------------|------|------------|------|------|\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-04-29 | 1    | 100  |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-06 | 2    | 99   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-13 | 3    | 96   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-20 | 4    | 76   |\n",
    "| 2000 | Nelly           | Country Grammar    | 4:17 | 2000-05-27 | 5    | 55   |\n",
    "\n",
    "\n",
    "**After Transformation: Two Separate Tables**\n",
    "\n",
    "1. **Track Table** (One row per unique track):\n",
    "\n",
    "| id | artist          | track              | time |\n",
    "|----|-----------------|--------------------|------|\n",
    "| 1  | Nelly           | Country Grammar    | 4:17 |\n",
    "\n",
    "2. **Rank Table** (One row per weekly ranking):\n",
    "\n",
    "| id | date       | rank |\n",
    "|----|------------|------|\n",
    "| 1  | 2000-04-29 | 100  |\n",
    "| 1  | 2000-05-06 | 99   |\n",
    "| 1  | 2000-05-13 | 96   |\n",
    "| 1  | 2000-05-20 | 76   |\n",
    "| 1  | 2000-05-27 | 55   |"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4120c61",
   "metadata": {},
   "source": [
    "# Import the billboard DataFrame from exercise 1.2 and ensure it is in long format.\n",
    "\n",
    "# Extract unique tracks along with their artist and duration\n",
    "unique_tracks = billboard_cleaned[[\"artist.inverted\", \"track\", \"time\"]].drop_duplicates()\n",
    "\n",
    "# Assign a unique ID to each track\n",
    "unique_tracks.insert(0, \"track_id\", range(1, len(unique_tracks) + 1))\n",
    "\n",
    "# Merge the original DataFrame with the unique tracks, adding track IDs to each record\n",
    "billboard_with_track_id = pd.merge(billboard_cleaned, unique_tracks, on=[\"artist.inverted\", \"track\", \"time\"])\n",
    "\n",
    "# Retain only the relevant columns: track ID, date, and rank\n",
    "billboard_with_track_id = billboard_with_track_id[[\"track_id\", \"date\", \"rank\"]]\n",
    "\n",
    "# Merge data for a specific artist (e.g., \"Nelly\") with track details\n",
    "pd.merge(\n",
    "    unique_tracks[unique_tracks[\"artist.inverted\"] == \"Nelly\"],\n",
    "    billboard_with_track_id,\n",
    "    on=[\"track_id\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1dd02579",
   "metadata": {},
   "source": [
    "### Exercise 3: Visualizing the Palmer Penguins Dataset with Seaborn\n",
    "\n",
    "The Palmer Penguins dataset provides information about three penguin species—Adélie, Chinstrap, and Gentoo—observed in Antarctica. This dataset contains details such as body mass, flipper length, and bill dimensions, making it ideal for data visualization practice.\n",
    "\n",
    "#### Dataset Overview\n",
    "The dataset includes the following key columns:\n",
    "- `species`: The penguin species (Adélie, Chinstrap, Gentoo).\n",
    "- `island`: The island where the penguins were observed.\n",
    "- `bill_length_mm`: Length of the penguin's bill (in millimeters).\n",
    "- `bill_depth_mm`: Depth of the penguin's bill (in millimeters).\n",
    "- `flipper_length_mm`: Length of the penguin's flipper (in millimeters).\n",
    "- `body_mass_g`: Body mass of the penguin (in grams).\n",
    "- `sex`: The penguin's sex (male, female).\n",
    "\n",
    "The dataset can be loaded directly from the seaborn repository using the URL with `pd.read_csv(url)`:  \n",
    "<https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv>\n",
    "\n",
    "In the previous tutorial, we briefly explored how to create plots using **matplotlib**. Today, we will focus on **seaborn**, a powerful Python library built on top of matplotlib that allows us to create elegant and informative visualizations with minimal effort. By using seaborn, we can generate complex and aesthetically pleasing plots with just a few lines of code.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "1. **Visualize the Distribution of Flipper Length**:\n",
    "   - Create a histogram or kernel density plot (KDE) of `flipper_length_mm`, with the distributions differentiated by `species` using the `hue` parameter.\n",
    "\n",
    "2. **Explore the Relationship Between Body Mass and Flipper Length**:\n",
    "   - Use a scatter plot to visualize the relationship between `body_mass_g` and `flipper_length_mm`.\n",
    "   - Differentiate the points by `species` using the `hue` parameter and use distinct markers for each species.\n",
    "\n",
    "3. **Compare Body Mass Across Species**:\n",
    "   - Create a boxplot to compare the distributions of `body_mass_g` across the three penguin species.\n",
    "\n",
    "4. **Analyze Pairwise Relationships**:\n",
    "   - Generate a pair plot for the numerical variables (`bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`), with the points colored by `species`. Use a KDE plot for the diagonal.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "552d9779",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Palmer Penguins dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv\"\n",
    "penguins = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "display(penguins.head())\n",
    "\n",
    "# 1. Distribution of Flipper Length\n",
    "sns.histplot(\n",
    "    data=penguins, \n",
    "    x=\"flipper_length_mm\", \n",
    "    hue=\"species\", \n",
    "    kde=True, \n",
    "    palette=\"pastel\",\n",
    "    bins=20\n",
    ")\n",
    "plt.title(\"Distribution of Flipper Length by Species\")\n",
    "plt.xlabel(\"Flipper Length (mm)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Relationship Between Body Mass and Flipper Length\n",
    "sns.scatterplot(\n",
    "    data=penguins, \n",
    "    x=\"flipper_length_mm\", \n",
    "    y=\"body_mass_g\", \n",
    "    hue=\"species\", \n",
    "    style=\"species\", \n",
    "    palette=\"Set2\"\n",
    ")\n",
    "plt.title(\"Body Mass vs. Flipper Length by Species\")\n",
    "plt.xlabel(\"Flipper Length (mm)\")\n",
    "plt.ylabel(\"Body Mass (g)\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Comparing Body Mass Across Species\n",
    "sns.boxplot(\n",
    "    data=penguins, \n",
    "    x=\"species\", \n",
    "    y=\"body_mass_g\", \n",
    "    palette=\"viridis\", \n",
    "    hue=\"species\"\n",
    ")\n",
    "plt.title(\"Body Mass Distribution Across Species\")\n",
    "plt.xlabel(\"Species\")\n",
    "plt.ylabel(\"Body Mass (g)\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Pair Plot of Numerical Variables\n",
    "sns.pairplot(\n",
    "    data=penguins, \n",
    "    hue=\"species\", \n",
    "    palette=\"husl\", \n",
    "    diag_kind=\"kde\", \n",
    "    markers=[\"o\", \"s\", \"D\"]\n",
    ")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-course-2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
